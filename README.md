# Software-Observability

**What are Logs?**

<h2>Logs</h2>

<p>
Logs contain <strong>plain text or JSON</strong> to record events that occur during the processing of an
<strong>API request</strong> within an application.
</p>

<h3>Why Logs Are Important</h3>

<p>Logs can record the following levels:</p>

<ul>
  <li><strong>Information</strong></li>
  <li><strong>Warning</strong></li>
  <li><strong>Error</strong></li>
</ul>

<p>These are crucial for:</p>

<ul>
  <li>
    <strong>Debugging</strong> — understanding what happened within the application while processing an API request.
  </li>
  <li>
    <strong>Tracking errors and failures</strong> along with stack traces.
  </li>
  <li>
    <strong>Monitoring application behavior</strong> to detect unusual patterns.
  </li>
</ul>

<h3>API-Level Details Captured in Logs</h3>

<ul>
  <li>
    <strong>Incoming request details</strong>
    <ul>
      <li>URL</li>
      <li>HTTP method</li>
      <li>Headers</li>
      <li>Request body</li>
    </ul>
  </li>
  <li>
    <strong>Events occurring during API request processing</strong>
  </li>
  <li>
    <strong>Outgoing response details</strong>
    <ul>
      <li>Status code</li>
      <li>Time taken</li>
      <li>Response payload</li>
    </ul>
  </li>
</ul>

<h2>Limitation</h2>

<ul>
  <li>
    <strong>Logs</strong> do not show the <strong>API request end-to-end journey</strong> across microservices.
  </li>
  <li>
    We don’t know the <strong>time breakdown per service</strong>, which helps us understand where the request spent more time.
  </li>
</ul>

<p>
If a request goes through:
<strong>OrderService → PaymentService</strong>
</p>

<p>
Each application maintains its logs <strong>independently</strong>.
</p>

<pre>
[OrderService] Order created for userId=55
</pre>

<p>Similarly:</p>

<pre>
[PaymentService] Payment timeout userId=55
</pre>

<p>
But we can’t easily <strong>tie these logs together</strong> (for example, whether both logs belong to the same API request)
or determine <strong>how long each step took</strong>.
</p>

<img width="1378" height="670" alt="image" src="https://github.com/user-attachments/assets/46c03387-485c-44c1-b118-4fdf4dd7ac91" />

<h2>Tracing</h2>

<ul>
  <li>
    <strong>Tracing</strong> tracks the path of a request across <strong>microservices</strong>.
  </li>
</ul>

<p><strong>It shows high-level information such as:</strong></p>

<ul>
  <li>Which service called which service</li>
  <li>How much time each service took</li>
  <li>Status code of each service (200, 400, 500, etc.)</li>
</ul>

<p>
<strong>But tracing does NOT store detailed logs.</strong>
</p>

<h2>How Distributed Tracing Works</h2>

<p>
To achieve this, <strong>Distributed Tracing</strong> creates
<strong>TRACE_ID</strong> and <strong>SPAN_ID</strong>.
</p>

<ul>
  <li>
    <strong>Trace ID</strong>:
    Connects a request across multiple microservices using
    <strong>one unique identifier</strong>.
  </li>
</ul>

<ul>
  <li>
    <strong>Span ID</strong>:
    Each service has its own unique <strong>Span ID</strong>, which represents
    individual units of work performed inside that service.
  </li>
</ul>

<p>Let's say everything in here is a synchronous call. Service A is waiting till all the internal processes are done, and a response can be returned from Service A's end. So, we now know which service took how much time.</p>

<img width="1577" height="676" alt="image" src="https://github.com/user-attachments/assets/bf79c3e4-47ca-4cb2-b507-b124c55b008f" />

<p>We can make our own custom span and inside that a child span, just to know when a particular operation started and when it ended. A child span will be part of the parent span, which means the task inside a task and how much time it took for that child task to execute will be observed with the help of the child span.</p>

 <strong>In general, mentioned below are those for which Springboot microservice internally creates a new span</strong>:

<ul>
  <li>When we have <strong>an incoming request</strong> to an application.</li>
    <li>When we create <strong>a new thread or thread handoffs</strong>.</li>
      <li>Within an application (When having at least two applications) <strong>trying to invoke a second application</strong>, so for a duration from when the request is initiated for invoking the second application till the response is returned</li>
</ul>

<img width="1307" height="303" alt="image" src="https://github.com/user-attachments/assets/5a733a3f-0b80-46e3-8026-956aefca25f0" />

<h2>Note</h2>

<p>
A modern <strong>distributed logging system</strong> uses the
<strong>Trace ID</strong> generated by <strong>distributed tracing</strong>
to stitch together logs from multiple microservices that belong to the
<strong>same API request</strong>.
</p>

<p>
If a request flows through:
<strong>OrderService → PaymentService</strong>
</p>

<p>
Each application maintains its logs <strong>independently</strong>, but the logs
are stitched together using a <strong>unique and common Trace ID</strong>.
</p>

<pre>
TRACE-ID=T123 [OrderService] Order created for userId=55
</pre>

<p>Similarly:</p>

<pre>
TRACE-ID=T123 [PaymentService] Payment timeout userId=55
</pre>

<p>
Even though these are <strong>separate logs</strong> from different services,
they can be combined in tools like <strong>Kibana</strong> or
<strong>CloudWatch</strong>.
</p>

<p>
For example, when we search using
<strong>traceId = T123</strong>, all related logs from different services involved
in a single API request appear together.
</p>
<h2>Old / Legacy Flow</h2>

<ul>
  <li>
    Uses <strong>Spring Cloud Sleuth</strong>, which relies on the
    <strong>Brave</strong> library and is compatible with the
    <strong>Zipkin</strong> backend service.
  </li>
  <li>
    Supporting other backend services like <strong>Jaeger</strong> is
    <strong>not straightforward</strong> and may require additional
    configuration hacks.
  </li>
</ul>

<img width="1141" height="373" alt="image" src="https://github.com/user-attachments/assets/43f80169-f9a5-4a19-bbac-bbb4a66ab9b6" />

<h2>Distributed Tracing with Spring Cloud Sleuth and Zipkin</h2>

<p>
When a new API request enters the system, <strong>Spring Cloud Sleuth</strong>
automatically takes responsibility for generating all the required tracing
information.
</p>

<h3>Trace and Span Generation</h3>

<p>
For every fresh request, Sleuth generates a <strong>Trace ID</strong> and an
initial <strong>Span ID</strong>.
</p>

<ul>
  <li>
    The <strong>Trace ID</strong> uniquely identifies the request across all
    microservices.
  </li>
  <li>
    The <strong>Span ID</strong> represents a single unit of work within a
    service.
  </li>
</ul>

<p>
As the request propagates to downstream microservices, Sleuth automatically
adds the <strong>Trace ID</strong> and <strong>Parent Span ID</strong> to the
<strong>request headers</strong>.
</p>

<p>
This ensures that:
</p>

<ul>
  <li>The downstream service does <strong>not generate a new Trace ID</strong></li>
  <li>The new span is correctly linked to its <strong>parent span</strong></li>
  <li>The complete request flow remains connected across services</li>
</ul>

<h3>Span Lifecycle</h3>

<p>
Each service creates its own span to represent the work it performs.
Every span contains at least the following information:
</p>

<ul>
  <li><strong>Trace ID</strong></li>
  <li><strong>Span ID</strong></li>
  <li><strong>Parent Span ID</strong></li>
</ul>

<p>
In addition to these, spans can include extra metadata such as:
</p>

<ul>
  <li>Request URL</li>
  <li>HTTP method</li>
  <li>Service name</li>
  <li>Execution time</li>
  <li>Error details (if any)</li>
</ul>

<p>
Once a span is completed, <strong>Spring Cloud Sleuth</strong> places it into an
<strong>asynchronous in-memory queue</strong>.  
This process is handled internally by Sleuth to avoid blocking the main
request flow.
</p>

<h3>Zipkin Exporter</h3>

<p>
The <strong>Zipkin Exporter</strong> is responsible for sending completed spans
from the in-memory queue to the <strong>Zipkin backend</strong>.
</p>

<p>
It exports span data asynchronously, ensuring minimal performance impact on
the application.
</p>

<h3>Zipkin Backend</h3>

<p>
The <strong>Zipkin backend</strong> receives span data from multiple services
and builds a <strong>tree-like structure</strong> by stitching together
<strong>Trace IDs</strong> and <strong>Span IDs</strong>.
</p>

<p>
This stitched view allows Zipkin to:
</p>

<ul>
  <li>Visualize the complete request flow across microservices</li>
  <li>Show parent-child relationships between spans</li>
  <li>Display latency and timing information for each service</li>
  <li>Help identify bottlenecks and failures</li>
</ul>

<p>
Apart from Zipkin, other observability tools such as
<strong>Grafana</strong> can also be used for visualization and monitoring,
depending on the tracing backend and setup.
</p>
<h2>Distributed Tracing – Diagram Friendly Explanation</h2>

<p>
Distributed Tracing tracks the complete journey of an API request as it flows
through multiple microservices using a <strong>Trace ID</strong> and
<strong>Span IDs</strong>.
</p>

<h3>High-Level Flow</h3>

<pre>
Client Request
      |
      v
+----------------+
| Order Service  |
| Trace ID: T1   |
| Span ID: S1    |
+----------------+
      |
      v
+-------------------+
| Payment Service   |
| Trace ID: T1      |
| Span ID: S2       |
| Parent: S1        |
+-------------------+
      |
      v
+-------------------+
| Inventory Service |
| Trace ID: T1      |
| Span ID: S3       |
| Parent: S2        |
+-------------------+
</pre>

<p>
All services share the <strong>same Trace ID</strong>, but each service creates
its own <strong>Span ID</strong> to represent the work it performs.
</p>

---

<h2>Step-by-Step Request Flow Example</h2>

<h3>Step 1: Client Sends Request</h3>

<p>
A client sends a request to the <strong>Order Service</strong>.
Since this is a new request, <strong>Spring Cloud Sleuth</strong> generates:
</p>

<ul>
  <li>A new <strong>Trace ID</strong> (example: <code>T123</code>)</li>
  <li>An initial <strong>Span ID</strong> (example: <code>S1</code>)</li>
</ul>

<pre>
TRACE-ID=T123
SPAN-ID=S1
</pre>

---

<h3>Step 2: Order Service Processes the Request</h3>

<p>
The Order Service creates a span representing its internal processing.
Once completed, the span is placed into an
<strong>asynchronous in-memory queue</strong>.
</p>

<pre>
TRACE-ID=T123 [OrderService] Order created for userId=55
</pre>

---

<h3>Step 3: Request Propagates to Payment Service</h3>

<p>
When Order Service calls the Payment Service, Sleuth automatically propagates
the tracing information via request headers:
</p>

<ul>
  <li><strong>Trace ID: T123</strong></li>
  <li><strong>Parent Span ID: S1</strong></li>
</ul>

<p>
The Payment Service:
</p>

<ul>
  <li>Does <strong>not</strong> create a new Trace ID</li>
  <li>Creates a new Span ID (example: <code>S2</code>)</li>
  <li>Links the span to its parent (<code>S1</code>)</li>
</ul>

<pre>
TRACE-ID=T123
SPAN-ID=S2
PARENT-SPAN-ID=S1
</pre>

---

<h3>Step 4: Payment Service Completes Its Work</h3>

<p>
After processing, the Payment Service completes its span and places it into the
in-memory queue managed by Sleuth.
</p>

<pre>
TRACE-ID=T123 [PaymentService] Payment timeout userId=55
</pre>

---

<h3>Step 5: Zipkin Exporter Sends Spans</h3>

<p>
The <strong>Zipkin Exporter</strong> asynchronously picks completed spans from
the in-memory queue and sends them to the <strong>Zipkin backend</strong>.
</p>

<p>
This asynchronous export ensures that tracing does not impact application
performance.
</p>

---

<h3>Step 6: Zipkin Backend Builds the Trace</h3>

<p>
The Zipkin backend receives spans from all services and stitches them together
using:
</p>

<ul>
  <li>Trace ID</li>
  <li>Span ID</li>
  <li>Parent Span ID</li>
</ul>

<p>
This creates a <strong>tree-like visualization</strong> that shows:
</p>

<ul>
  <li>Which service called which service</li>
  <li>How much time each service took</li>
  <li>Where errors or delays occurred</li>
</ul>

<p>
Using tools like <strong>Zipkin UI</strong> or <strong>Grafana</strong>, engineers
can easily analyze and debug distributed systems.
</p>

<h2>Modern Flow</h2>

<p>Micrometer has replaced Spring Cloud Sleuth because Sleuth uses the Brave library very specific to Zipkin only, and a new backend could not be easily integrated with Sleuth; however, Micrometer is an interface only.</p>

<img width="689" height="595" alt="image" src="https://github.com/user-attachments/assets/5046f67f-0525-49ec-9645-0df662e1e041" />

<img width="1810" height="690" alt="image" src="https://github.com/user-attachments/assets/be457906-6282-431f-9014-c9f1616743b9" />

<p>OpenTelemetry provides an implementation of all those APIs of Micrometer. Even Brave does that, but in this case, we shall have to use the Zipkin backend. Whereas, OpenTelemetry is backend agnostic</p>

<h2>What is OTLP (OpenTelemetry Protocol)?</h2>

<p>
<strong>OTLP (OpenTelemetry Protocol)</strong> is a vendor-neutral, standardized
protocol used to <strong>send observability data</strong> such as
<strong>traces, metrics, and logs</strong> from applications to observability
backends.
</p>

<p>
It is part of the <strong>OpenTelemetry</strong> ecosystem and is designed to
replace vendor-specific formats with a <strong>single unified protocol</strong>.
</p>

<hr/>

<h3>Why OTLP is Needed</h3>

<p>
Before OTLP, each observability backend used its own data format and ingestion
mechanism:
</p>

<ul>
  <li>Zipkin had its own trace format</li>
  <li>Jaeger had a different ingestion format</li>
  <li>Metrics and logs required separate exporters</li>
</ul>

<p>
This made it difficult to switch tools or send the same data to multiple
backends.
</p>

<p>
<strong>OTLP solves this problem</strong> by acting as a common language between
applications and observability platforms.
</p>

<hr/>

<h3>What Data Does OTLP Carry?</h3>

<ul>
  <li><strong>Traces</strong> (Trace ID, Span ID, Parent Span ID, latency, errors)</li>
  <li><strong>Metrics</strong> (counters, gauges, histograms)</li>
  <li><strong>Logs</strong> (structured logs with context)</li>
</ul>

<p>
OTLP supports both:
</p>

<ul>
  <li><strong>gRPC</strong> (default, efficient, binary)</li>
  <li><strong>HTTP/JSON</strong> (human-readable, easier debugging)</li>
</ul>

<hr/>

<h2>How OTLP Works (High-Level Flow)</h2>

<pre>
Application
   |
   |  (OTLP: Traces / Metrics / Logs)
   v
OpenTelemetry SDK
   |
   v
OpenTelemetry Collector
   |
   +----> Zipkin
   |
   +----> Jaeger
   |
   +----> Grafana (Tempo / Mimir / Loki)
</pre>

<p>
The application sends observability data in OTLP format to the
<strong>OpenTelemetry Collector</strong>, which then forwards it to one or more
backends.
</p>

<hr/>

<h2>How Zipkin Uses OTLP</h2>

<p>
<strong>Zipkin</strong> is primarily a distributed tracing backend.
</p>

<p>
Modern Zipkin setups can:
</p>

<ul>
  <li>Accept traces via <strong>OTLP</strong></li>
  <li>Convert OTLP trace data into Zipkin’s internal format</li>
  <li>Visualize traces as request trees</li>
</ul>

<p>
Flow:
</p>

<pre>
Application → OTLP → OpenTelemetry Collector → Zipkin
</pre>

<p>
Zipkin uses OTLP data to:
</p>

<ul>
  <li>Build parent-child span relationships</li>
  <li>Calculate service latencies</li>
  <li>Identify failures and bottlenecks</li>
</ul>

<hr/>

<h2>How Jaeger Uses OTLP</h2>

<p>
<strong>Jaeger</strong> is also a distributed tracing backend, but it is
<strong>natively aligned with OpenTelemetry</strong>.
</p>

<p>
Jaeger can:
</p>

<ul>
  <li>Directly ingest <strong>OTLP traces</strong></li>
  <li>Store them in its internal trace storage</li>
  <li>Display end-to-end request flows</li>
</ul>

<p>
Flow:
</p>

<pre>
Application → OTLP → OpenTelemetry Collector → Jaeger
</pre>

<p>
Because OTLP is the default OpenTelemetry protocol, Jaeger integration is
<strong>simpler and more future-proof</strong> compared to legacy Zipkin-only
setups.
</p>

<hr/>

<h2>How Grafana Uses OTLP</h2>

<p>
<strong>Grafana</strong> is not a single backend but a complete observability
platform that works with multiple storage systems.
</p>

<p>
Grafana uses OTLP with different components:
</p>

<ul>
  <li><strong>Grafana Tempo</strong> → Traces (OTLP)</li>
  <li><strong>Grafana Mimir / Prometheus</strong> → Metrics (OTLP)</li>
  <li><strong>Grafana Loki</strong> → Logs (OTLP or structured logs)</li>
</ul>

<p>
Flow:
</p>

<pre>
Application
   |
   v
OTLP
   |
   v
OpenTelemetry Collector
   |
   +----> Grafana Tempo (Traces)
   +----> Grafana Mimir (Metrics)
   +----> Grafana Loki (Logs)
</pre>

<p>
Grafana then provides a <strong>single UI</strong> to correlate:
</p>

<ul>
  <li>Traces ↔ Logs ↔ Metrics</li>
  <li>Errors ↔ Latency ↔ Resource usage</li>
</ul>

<hr/>

<h2>Why OTLP is Important</h2>

<ul>
  <li>Vendor-neutral and future-proof</li>
  <li>Single protocol for traces, metrics, and logs</li>
  <li>Easy backend switching (Zipkin ↔ Jaeger ↔ Grafana)</li>
  <li>Enables true observability correlation</li>
</ul>

<p>
<strong>OTLP is the foundation of modern observability.</strong>
</p>

<h2>Implementation</h2>

<h3>Step 1: Choose a Tracing Backend and Start the Server</h3>

<p>
The first step in implementing distributed tracing is to choose a
<strong>tracing backend</strong> that will collect, store, and visualize traces.
</p>

<p>
In this setup, we are using <strong>Jaeger</strong> as the tracing backend.
</p>

<h3>Running Jaeger Using Docker</h3>

<p>
Jaeger provides an <strong>all-in-one</strong> Docker image that includes:
</p>

<ul>
  <li>Jaeger UI</li>
  <li>Collector</li>
  <li>Query service</li>
  <li>Storage</li>
</ul>

<p>
We can use Docker to install and run Jaeger easily:
</p>

<pre>
docker run -d \
  -p 16686:16686 \
  -p 4317:4317 \
  -p 4318:4318 \
  jaegertracing/all-in-one:latest
</pre>

<h3>What Docker Does Internally</h3>

<p>
When this command is executed, Docker performs the following steps:
</p>

<ol>
  <li>Checks if the Jaeger image exists locally</li>
  <li>If not found, pulls the image from <strong>Docker Hub</strong></li>
  <li>Creates a new container from the image</li>
  <li>Runs the container in detached mode (<code>-d</code>)</li>
  <li>Exposes the required ports to the host machine</li>
</ol>

<h3>Exposed Ports</h3>

<ul>
  <li>
    <strong>16686</strong> – Jaeger UI  
    <br/>
    Open in browser:
    <a href="http://localhost:16686" target="_blank">
      http://localhost:16686
    </a>
  </li>
  <li>
    <strong>4317</strong> – OTLP gRPC endpoint  
    <br/>
    Used by applications and OpenTelemetry Collector to send trace data via gRPC
  </li>
  <li>
    <strong>4318</strong> – OTLP HTTP endpoint  
    <br/>
    Used for sending trace data over HTTP/JSON
  </li>
</ul>

<p>
Once the container is running, Jaeger is ready to receive
<strong>OTLP trace data</strong> from your applications or from the
<strong>OpenTelemetry Collector</strong>.
</p>

<h2>Application setup</h2>

<img width="1036" height="442" alt="image" src="https://github.com/user-attachments/assets/35ea276d-81bf-45c5-ae83-5ec50bbaa0eb" />

<h2>Application Configuration (application.properties)</h2>

<p>
This section explains the configuration required to enable
<strong>distributed tracing</strong> in a Spring Boot application using
<strong>Micrometer</strong>, <strong>OpenTelemetry (OTLP)</strong>, and
<strong>Jaeger</strong>.
</p>

<hr/>

<h3>Server Configuration</h3>

<pre>
server.port=8080
</pre>

<p>
Defines the port on which the Spring Boot application will run.
In this case, the application will be accessible on:
</p>

<pre>
http://localhost:8080
</pre>

<hr/>

<h3>Application Name</h3>

<pre>
spring.application.name=Trace-App-1
</pre>

<p>
This sets the <strong>service name</strong> for the application.
</p>

<p>
The service name is important because:
</p>

<ul>
  <li>It appears in the tracing backend (Jaeger UI)</li>
  <li>It helps identify which service generated a particular span</li>
  <li>It is used to group traces by service</li>
</ul>

<hr/>

<h3>Micrometer Tracing Configuration</h3>

<pre>
management.tracing.sampling.probability=1.0
</pre>

<p>
This property controls the <strong>sampling rate</strong> for tracing.
</p>

<ul>
  <li>
    <strong>1.0</strong> means <strong>100% of requests are traced</strong>
  </li>
  <li>
    A lower value (for example <code>0.1</code>) would trace only 10% of requests
  </li>
</ul>

<p>
In development or learning environments, keeping this value at
<strong>1.0</strong> is useful to ensure that every request appears in Jaeger.
</p>

<p>
In production environments, this value is usually reduced to avoid performance
overhead.
</p>

<hr/>

<h3>OTLP Exporter Configuration (Jaeger)</h3>

<pre>
management.otlp.tracing.endpoint=http://localhost:4318/v1/traces
</pre>

<p>
This property configures the <strong>OTLP exporter</strong>, which is responsible
for sending trace data (spans) from the application to the tracing backend.
</p>

<p>
Explanation:
</p>

<ul>
  <li>
    <strong>http://localhost:4318</strong>  
    → OTLP HTTP endpoint exposed by the Jaeger all-in-one container
  </li>
  <li>
    <strong>/v1/traces</strong>  
    → OTLP API path used specifically for sending trace data
  </li>
</ul>

<p>
Whenever a span is completed:
</p>

<ul>
  <li>Micrometer collects the span data</li>
  <li>The OTLP exporter sends it to this endpoint</li>
  <li>Jaeger receives and stores the span</li>
</ul>

<hr/>

<h3>End-to-End Flow</h3>

<pre>
Incoming Request
      |
      v
Spring Boot Application
      |
      v
Micrometer Tracing
      |
      v
OTLP Exporter
      |
      v
Jaeger Backend (http://localhost:4318/v1/traces)
      |
      v
Jaeger UI (http://localhost:16686)
</pre>

<p>
As a result, every request handled by this application is traced and visualized
in the Jaeger UI with complete timing and service information.
</p>

<hr/>

<h3>Summary</h3>

<ul>
  <li>The application runs on port <strong>8080</strong></li>
  <li>The service name is set to <strong>Trace-App-1</strong></li>
  <li>All requests are traced (<strong>100% sampling</strong>)</li>
  <li>Spans are exported using <strong>OTLP over HTTP</strong></li>
  <li>Jaeger acts as the tracing backend and visualization tool</li>
</ul>

<h3>Tracing Sampling Configuration</h3>

<pre>
management.tracing.sampling.probability=1.0
</pre>

<p>
This property controls the <strong>sampling rate</strong> for distributed
tracing in a Spring Boot application using <strong>Micrometer Tracing</strong>.
</p>

<p>
Sampling determines <strong>how many incoming requests are traced</strong> and
sent to the tracing backend (for example, Jaeger).
</p>

<hr/>

<h4>What Does a Value of 1.0 Mean?</h4>

<p>
The value <strong>1.0</strong> represents a <strong>100% sampling rate</strong>.
</p>

<ul>
  <li>Every incoming request is traced</li>
  <li>A Trace ID and Span IDs are created for each request</li>
  <li>All spans are exported to the tracing backend</li>
</ul>

<p>
This ensures that <strong>no request is skipped</strong> in tracing.
</p>

<hr/>

<h4>How Sampling Works Internally</h4>

<p>
When a request enters the application:
</p>

<ol>
  <li>Micrometer checks the sampling probability</li>
  <li>A random decision is made based on the configured value</li>
  <li>If the request is sampled, tracing data is generated</li>
  <li>If not sampled, no spans are created</li>
</ol>

<p>
With a probability of <strong>1.0</strong>, this decision always evaluates to
<strong>true</strong>, meaning tracing is always enabled.
</p>

<hr/>

<h4>Examples of Different Sampling Values</h4>

<table border="1" cellpadding="8" cellspacing="0">
  <thead>
    <tr>
      <th>Sampling Value</th>
      <th>Meaning</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1.0</td>
      <td>Trace 100% of requests</td>
    </tr>
    <tr>
      <td>0.5</td>
      <td>Trace approximately 50% of requests</td>
    </tr>
    <tr>
      <td>0.1</td>
      <td>Trace approximately 10% of requests</td>
    </tr>
    <tr>
      <td>0.01</td>
      <td>Trace approximately 1% of requests</td>
    </tr>
  </tbody>
</table>

<p>
Lower values reduce the volume of tracing data but may miss rare or
intermittent issues.
</p>

<hr/>

<h4>Why Use 1.0 in Development?</h4>

<ul>
  <li>Ensures all requests appear in the tracing UI</li>
  <li>Makes learning and debugging easier</li>
  <li>Helps verify correct trace propagation across services</li>
</ul>

<p>
For tutorials, demos, and local development,
<strong>1.0 is strongly recommended</strong>.
</p>

<hr/>

<h4>Why Not Use 1.0 in Production?</h4>

<p>
Tracing every request in a high-traffic production system can cause:
</p>

<ul>
  <li>Increased CPU and memory usage</li>
  <li>Higher network overhead due to span exports</li>
  <li>Larger storage requirements in the tracing backend</li>
</ul>

<p>
For production systems, a lower sampling rate is usually preferred.
</p>

<hr/>

<h4>Best Practices</h4>

<ul>
  <li>Use <strong>1.0</strong> in local and test environments</li>
  <li>Use <strong>0.1</strong> or lower in production</li>
  <li>Increase sampling temporarily during incident investigation</li>
  <li>Combine sampling with error-based or adaptive sampling if supported</li>
</ul>

<p>
Proper sampling helps balance <strong>observability</strong> and
<strong>system performance</strong>.
</p>


<img width="986" height="469" alt="image" src="https://github.com/user-attachments/assets/438912b4-259d-4392-b4d4-2ba460fd96b6" />

<h4>That is all we need:</h4>
<p>Now tracing automatically initializes and starts generating spans for:</p>
<ul>
  <li> <strong>Incoming HTTP requests</strong>.</li>
  <li><strong>Outgoing HTTP requests</strong>.</li>
  <li>Thread Handoffs.</li>
  <li>Propagation of requests, etc.</li>
</ul>

<img width="1626" height="567" alt="image" src="https://github.com/user-attachments/assets/aa5a2169-aeb9-47b1-ba11-b5a20066b4a0" />

<img width="984" height="610" alt="image" src="https://github.com/user-attachments/assets/4c7858de-67d5-4251-a837-34d8c3f4798d" />

<img width="1675" height="872" alt="image" src="https://github.com/user-attachments/assets/1b1602c8-eb1d-44b1-9275-82b1363587fb" />

<img width="1746" height="713" alt="image" src="https://github.com/user-attachments/assets/019085b7-aad8-4656-8790-87a0c8b8cd66" />

<img width="903" height="525" alt="image" src="https://github.com/user-attachments/assets/7506eb70-fde6-420c-8166-f251437576fb" />

<img width="1771" height="893" alt="image" src="https://github.com/user-attachments/assets/9e80c468-27cb-482f-824a-540184f580b6" />

<img width="507" height="211" alt="image" src="https://github.com/user-attachments/assets/f6f99670-408b-4e55-ad41-7b16d29e1436" />

<img width="980" height="400" alt="image" src="https://github.com/user-attachments/assets/dc98c509-83d7-4915-b739-ad8e7aa53d35" />

<img width="1577" height="352" alt="image" src="https://github.com/user-attachments/assets/ed406af2-d407-47f5-b8b4-c7c12de63084" />

<img width="1511" height="937" alt="image" src="https://github.com/user-attachments/assets/b0807b5c-8f37-4b66-98e4-600c6bf42fcb" />

<p>Once we hit the above-mentioned endpoint, that will invoke the application2 endpoint as well.</p>

<img width="1900" height="839" alt="image" src="https://github.com/user-attachments/assets/6d5c69b7-4dc1-456d-a370-35c147b0c12b" />

<p>This is how the trace would look:</p>

<img width="1902" height="415" alt="image" src="https://github.com/user-attachments/assets/4a522a38-5ffd-4eed-9370-4aba93751399" />

<img width="1802" height="526" alt="image" src="https://github.com/user-attachments/assets/89985427-60d5-4283-9f6a-ba8ac57c2102" />

<p>We can see that, from App2 invocation time till the request is complete, the span is larger than the actual processing time within App2. Because the span in App1 measures the entire remote call, not just App2’s internal business logic.</p>

<h2>Note: Difference Between App1 and App2 Span Duration</h2>

<p>
In distributed tracing, it is <strong>expected behavior</strong> that the span
duration of the calling service (<strong>App1</strong>) is
<strong>greater</strong> than the actual processing time of the called service
(<strong>App2</strong>).
</p>

<h3>Reason</h3>

<p>
The span created in <strong>App1</strong> measures the
<strong>entire remote call lifecycle</strong>, not just the business logic
executed inside App2.
</p>

<p>
This includes:
</p>

<ul>
  <li>Request preparation in App1 (serialization, headers, tracing context)</li>
  <li>Network latency between App1 and App2</li>
  <li>Request handling and business logic execution in App2</li>
  <li>Network latency between App2 and App1</li>
  <li>Response handling in App1 (deserialization, client processing)</li>
   <li>network + framework + serialization + queuing + tracing overhead</li>
</ul>

<p>
In contrast, the span created in <strong>App2</strong> measures
<strong>only the internal processing time</strong> within App2.
</p>

<hr/>

<h3>Visual Representation</h3>

<pre>
App1 Client Span
┌───────────────────────────────────────────────┐
|  DNS | Connection | Serialization | Network   |
|-----------------------------------------------|
|        App2 Server Span                       |
|        ┌─────────────────────────────┐        |
|        | Controller + Business Logic |        |
|        └─────────────────────────────┘        |
|-----------------------------------------------|
| Network | Deserialization | Response  Handling|
└───────────────────────────────────────────────┘
</pre>

<p>
The App2 span is <strong>nested inside</strong> the App1 span, and the App1 span
represents the <strong>end-to-end latency</strong> of the remote call.
App1 span = total round-trip time
</p>

<hr/>

<h3>Example Timeline</h3>

<table border="1" cellpadding="8" cellspacing="0">
  <thead>
    <tr>
      <th>Phase</th>
      <th>Time (ms)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>App1 request preparation</td>
      <td>20</td>
    </tr>
    <tr>
      <td>Network (App1 → App2)</td>
      <td>40</td>
    </tr>
    <tr>
      <td>App2 processing</td>
      <td>30</td>
    </tr>
    <tr>
      <td>Network (App2 → App1)</td>
      <td>40</td>
    </tr>
    <tr>
      <td>Response handling in App1</td>
      <td>20</td>
    </tr>
    <tr>
      <td><strong>Total App1 span</strong></td>
      <td><strong>150</strong></td>
    </tr>
    <tr>
      <td><strong>App2 span only</strong></td>
      <td><strong>30</strong></td>
    </tr>
  </tbody>
</table>

<hr/>

<h3>Key Takeaway</h3>

<p>
The span duration in the calling service represents the
<strong>complete round-trip latency</strong>, while the span duration in the
called service represents <strong>only its internal execution time</strong>.
</p>

<p>
The difference between the two is caused by
<strong>network latency, client-side overhead, framework processing, and
serialization</strong>.
</p>


<p>If we do trace JSON, we will get something like</p>

<img width="1379" height="960" alt="image" src="https://github.com/user-attachments/assets/c3a3963a-7d59-4f3b-8d69-1fd0875c02a3" />






















